tokenizer_str: 'meta-llama/Llama-2-13b-hf'
trainer: {}
use_snr_optim: True
optimizers:
  - lr: 6e-4
    betas: [0.9, 0.95]
    target_modules:
      - 'encoder*.proj.models.*'
      - 'decoder*lora*'
      -
batch_size: 4
gradient_accumulation_steps: 4
epochs: 1
precision: 'fp16'
model:
  use_cross_attn: False
  use_soft_prompting: True
  no_repeat_n_grams: [ 2, 3, 4, 5 ]
  loose_match_decoder_state_dict: True
  vision_encoder_config:
    n_embd_out_vit: 5120
    n_cls: 16
    gate_sizes: [ 2560 ]
    refine_base_model: False
  decoder_config:
    model_str: 'meta-llama/Llama-2-13b-hf'
    use_cross_attn: False
    vocab_size: 32000
    extra_tokens: 0
    load_in_4bit: True
    use_auth_token: True
    prepare_for_kbit_training: True
    enable_gradient_checkpointing: True
    lora_spec:
      r: 16
      lora_alpha: 64
      lora_dropout: 0.1
      target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj']
      force_enable_update_modules: ['*.lm_head.*']
